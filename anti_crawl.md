## 遇到的反爬虫策略
1. 使用 scrapy 爬取第2页就会出现404。 
+ 解决办法：换策略
---
2. 使用 selenium 在爬取一段时间后会被网站禁止一段时间。
+ 解决办法：加随机数，使等待时间变随机; 外加换IP
---
3. 在标签的class类里面增加了混淆，后面会更改随机数，变化标签的类名。
+ 选择更通用的class类名
+ 正则表达式
```
soup.find_all(href=re.compile("elsie"), id='link1')
```
+ 使用CSS选择器，同时选择两个类名定位
```
soup.select('div.tvgenre.clear')
```
---
4. 某一页长时间加载不上，TimeOut显示。
+ 解决办法:增加重试机制
```
while True: retry
```
+ 系统脚本自调用
---
5. 同一个IP连续登陆157页及以上会被封IP
+ 解决办法：每一百多页换一个IP，动态IP代理
---
6. 黑洞，蜘蛛陷阱
+ 解决办法：蜂窝网络
